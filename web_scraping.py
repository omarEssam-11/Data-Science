# -*- coding: utf-8 -*-
"""web scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZI2M4LmW5zfWeVYCpr2GwiPPnbz1R2QB
"""

import requests
from bs4 import BeautifulSoup
import csv



date = input("Enter date in mm/dd/yyyy format: ").strip()
page = requests.get(f"https://www.yallakora.com/match-center/?date={date}")

# print(page.content) # return the page at a byte code format

# parsing transform the byte code to readable format

src = page.content
soup = BeautifulSoup(src, 'lxml')
# print(soup)

matcheslist = []
championship = soup.find_all('div', {'class' :'matchCard'})
# print(championship[0].find('h2').text.strip())

all_matches_data = championship[0].find_all('div', {'class': 'teamsData'})
# print(all_matches_data)

# titles of championships
all_titles = []
for i in championship:
    all_titles.append(i.find('h2').text.strip())
# print(all_titles)

# all_matches = {}
# for i in championship:
#   for j in i.find_all('div', {'class': 'teamsData'}):
#     all_matches[i.find('h2').text.strip()]['teamsA'].append(j.find('div', {'class': 'teams teamA'}.text.strip()))
#     all_matches[i.find('h2').text.strip()]['teamsB'].append(j.find('div', {'class': 'teams teamB'}.text.strip()))
#     all_matches[i.find('h2').text.strip()]['result'].append(j.find('div', {'class': 'MResult'}.text.strip()))

# print(all_matches)



all_matches = {}
for i in championship:
    title = i.find('h2').text.strip()  # Get the title for this championship
    if title not in all_matches:  # Check if the title exists in the dictionary
        all_matches[title] = {'teamsA': [], 'teamsB': [], 'result': [], 'time': []}  # Initialize it if it doesn't

    for j in i.find_all('div', {'class': 'teamsData'}):
        all_matches[title]['teamsA'].append(j.find('div', {'class': 'teams teamA'}).text.strip())
        all_matches[title]['teamsB'].append(j.find('div', {'class': 'teams teamB'}).text.strip())
        all_results = j.find('div', {'class': 'MResult'}).find_all('span', {'class': 'score'})
        result = (all_results[0].text.strip()) + "-" + all_results[1].text.strip()
        all_matches[title]['result'].append(result)
        all_matches[title]['time'].append(j.find('div', {'class': 'MResult'}).find('span', {'class': 'time'}).text.strip())

# print(all_matches)

import pandas as pd
# Convert dictionary to a list of rows
data = []
for championship, details in all_matches.items():
    for teamA, teamB, result in zip(details['teamsA'], details['teamsB'], details['result']):
        data.append([championship, teamA, teamB, result])

# Create a DataFrame
df = pd.DataFrame(data, columns=['Championship', 'Team A', 'Team B', 'Result'])

# Save to a CSV file
# df.to_csv('all_matches.csv', index=False)

# Print DataFrame for tabular display
# print(df.head())

import requests
from bs4 import BeautifulSoup
import pandas as pd
import calendar

def get_yearly_data(year):
    all_matches = {}

    for month in range(1, 13):  # Loop through all months (1 to 12)
        num_days = calendar.monthrange(year, month)[1]

        for day in range(1, num_days + 1):
            date_str = f"{month:02d}/{day:02d}/{year}"
            try:
                url = f"https://www.yallakora.com/match-center/?date={month}/{day}/{year}"
                page = requests.get(url)
                soup = BeautifulSoup(page.content, 'lxml')

                # Check for multiple match sections
                match_sections = soup.find_all('div', {'class': 'matchCard'})
                if not match_sections:
                    print(f"No matches found for {date_str}")
                    continue

                for match in match_sections:
                    if match is None:
                        continue

                    title = match.find('h2').text.strip()  # Get the title for this championship
                    if title not in all_matches:  # Check if the title exists in the dictionary
                        all_matches[title] = {'teamsA': [], 'teamsB': [], 'result': [], 'time': [], 'date': []}

                    teams_data = match.find_all('div', {'class': 'teamsData'})
                    if not teams_data:
                        print(f"No team data found for championship: {title}")
                        continue

                    for data in teams_data:
                        teamA = data.find('div', {'class': 'teams teamA'}).text.strip()
                        teamB = data.find('div', {'class': 'teams teamB'}).text.strip()

                        if "برشلونة" not in (teamA, teamB):
                            continue

                        result_elem = data.find('div', {'class': 'MResult'})
                        result = "N/A"
                        time = "N/A"
                        if result_elem:
                            all_results = result_elem.find_all('span', {'class': 'score'})
                            if len(all_results) == 2:
                                result = f"{all_results[0].text.strip()}-{all_results[1].text.strip()}"
                            time_elem = result_elem.find('span', {'class': 'time'})
                            time = time_elem.text.strip() if time_elem else "N/A"

                        all_matches[title]['teamsA'].append(teamA)
                        all_matches[title]['teamsB'].append(teamB)
                        all_matches[title]['result'].append(result)
                        all_matches[title]['time'].append(time)
                        all_matches[title]['date'].append(date_str)

            except Exception as e:
                print(f"Error on {date_str}: {e}")

    data = []
    for championship, details in all_matches.items():
        for date, teamA, teamB, result, time in zip(details['date'], details['teamsA'], details['teamsB'], details['result'], details['time']):
            data.append([date, championship, teamA, result, teamB, time])

    df = pd.DataFrame(data, columns=['Date', 'Championship', 'Team A', 'Result', 'Team B', 'Time'])
    df.to_excel('barca_all_matches_yearlyReport.xlsx', index=False)
    print(df.head())

# get_yearly_data(2023)





"""### **get all titles in all pages**"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from itertools import zip_longest

ll = []

for j in range(84):
    page_number = j # i will use it to navigate between pages
    page = requests.get(f'https://wuzzuf.net/search/jobs/?a=hpb%7Cspbg&q=data%20analysis&start={page_number}')

    src = page.content # get page contents
    soup = BeautifulSoup(src, 'lxml')   # using the parser

    xx = soup.find_all('h2', {'class': 'css-m604qf'})
# print(xx)

    for i in xx:
      ll.append(i.text.strip())
    print(f"page {page_number} done")



# print(ll)





"""..

"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
# from itertools import zip_longest

ll = []
titles = []
company = []
locations = []
skills = []
links = []
salaries = []

page_number = 0 # i will use it to navigate between pages
page = requests.get(f'https://wuzzuf.net/search/jobs/?a=hpb%7Cspbg&q=data%20analysis&start={page_number}')

src = page.content # get page contents
soup = BeautifulSoup(src, 'lxml')   # using the parser

job_titles = soup.find_all('h2', {'class': 'css-m604qf'})
job_company = soup.find_all('a', {'class': 'css-17s97q8'})
job_location = soup.find_all('span', {'class': 'css-5wys0k'})
job_exp = soup.find_all('div', {'class': 'css-y4udm8'}) # will need more digging
# xx = job_exp

for i in range(len(job_titles)):
    titles.append(job_titles[i].text.strip())
    links.append(job_titles[i].find('a')['href'])
    company.append(job_company[i].text.strip())
    locations.append(job_location[i].text.strip())
    skills.append(job_exp[i].text.strip())

for link in links:
    page = requests.get(link)
    src = page.content
    soup = BeautifulSoup(src, 'lxml')
    salaries.append((soup.find('span', {'class': 'css-4xky9y'})).text.strip())



# print(ll)


data = []

for i in range(len(titles)):
    data.append([titles[i], company[i], locations[i], skills[i], links[i], salaries[i]])

df = pd.DataFrame(data, columns=['Title', 'Company', 'Location', 'Skills required', 'Links', 'Salary'])

df.to_excel('Data Science WUZZUF Page-1.xlsx', index=False)

import requests
from bs4 import BeautifulSoup
import pandas as pd
# from itertools import zip_longest

ll = []
titles = []
company = []
locations = []
skills = []
links = []
salaries = []

page_number = 0 # i will use it to navigate between pages
page = requests.get(f'https://wuzzuf.net/search/jobs/?a=hpb%7Cspbg&q=data%20analysis&start={page_number}')

src = page.content # get page contents
soup = BeautifulSoup(src, 'lxml')   # using the parser

job_titles = soup.find_all('h2', {'class': 'css-m604qf'})
job_company = soup.find_all('a', {'class': 'css-17s97q8'})
job_location = soup.find_all('span', {'class': 'css-5wys0k'})
job_exp = soup.find_all('div', {'class': 'css-y4udm8'}) # will need more digging
# xx = job_exp

for i in range(len(job_titles)):
    titles.append(job_titles[i].text.strip())
    links.append(job_titles[i].find('a')['href'])
    company.append(job_company[i].text.strip())
    locations.append(job_location[i].text.strip())
    skills.append(job_exp[i].text.strip())

for link in links:
    page = requests.get(link)
    src = page.content
    soup = BeautifulSoup(src, 'lxml')
    salary_element = soup.find('span', {'class': 'css-4xky9y'}) # find the salary element
    if salary_element: # check if salary element exists
        salaries.append(salary_element.text.strip())
    else:
        salaries.append('N/A') # append N/A if salary info is not found


# print(ll)


data = []

for i in range(len(titles)):
    data.append([titles[i], company[i], locations[i], skills[i], links[i], salaries[i]])

df = pd.DataFrame(data, columns=['Title', 'Company', 'Location', 'Skills required', 'Links', 'Salary'])

df.to_excel('Data Science WUZZUF Page-1.xlsx', index=False)

